{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingestion of data in S3 from the CORE API stored data as JSONs with up to 100 search results stored in each file. \n",
    "Per [BlazingText Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html), the algorithm requires each line of the input file should contain a single sentence of space separated tokens. Raw data will need to be processed to accomodate the training format. \n",
    "* This notebook extracts text from the JSON and stores it in S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from io import StringIO\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re, string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer as netlem\n",
    "lem = netlem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_bucket_name = 'core0823'\n",
    "stg_bucket = 'core0823-stg'\n",
    "stg_catalog_bucket = stg_bucket + '/Catalog'\n",
    "stg_bt_bucket = stg_bucket + '/BT_STG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [i['Key'] for i in s3_client.list_objects(Bucket=core_bucket_name)['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_file_location(f_bucket, f_file):\n",
    "    \"\"\"\n",
    "    Simply returns a formatted string with the S3 file location\n",
    "    \"\"\"\n",
    "    data_location = 's3://{}/{}'.format(f_bucket,f_file)\n",
    "    return data_location\n",
    "\n",
    "def json_file_to_dict(f_bucket, f_json_file):\n",
    "    \"\"\"\n",
    "    Intakes bucket and json file.\n",
    "    Returns dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_s3_obj = s3_client.get_object( Bucket= f_bucket, Key = f_json_file )\n",
    "        tmp_str_json = json_s3_obj['Body'].read().decode('utf-8')\n",
    "        fnl_json = ast.literal_eval(tmp_str_json)\n",
    "        return fnl_json\n",
    "    except:\n",
    "        pass \n",
    "        print('Fail importing json file/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_text_parse(f_bucket, f_file_name):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and file name. \n",
    "    Parses CORE API JSON.     \n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    tmp_file = json_file_to_dict(f_bucket, f_file_name)\n",
    "    if tmp_file is not None and tmp_file['data'] is not None:\n",
    "        for item in tmp_file['data']:\n",
    "            if item['_source']['description'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['description'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "\n",
    "            if item['_source']['fullText'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['fullText'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "            \n",
    "    return results_list\n",
    "\n",
    "\n",
    "def json_extract_text(f_bucket, f_file_list):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and list of JSON files from CORE API. \n",
    "    Parses CORE API JSON. \n",
    "    This function iterates over a list of files, where json_text_parse is for a single file.\n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    print('JSON extract text starting at: {}'.format(t0))\n",
    "    results_list = []\n",
    "    for file in f_file_list:\n",
    "        tmp_results = json_text_parse(f_bucket, file)\n",
    "        results_list.extend(tmp_results)\n",
    "    \n",
    "    results_list = [sent for sent in results_list if len(sent) > 1]\n",
    "    t1 = time.time()\n",
    "    print('JSON extract text complete at: {}'.format(t1))\n",
    "    print('JSON extract text tool: {}'.format(t1-t0))\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON extract text starting at: 1599555295.939915\n",
      "JSON extract text complete at: 1599555684.5175335\n",
      "JSON extract text tool: 388.577618598938\n"
     ]
    }
   ],
   "source": [
    "sentences = json_extract_text(core_bucket_name, json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentences_serialized = pickle.dumps(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.put_object(Body=sentences_serialized, Bucket='core0823-stg', Key='BT_STG/sentences.txt')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
