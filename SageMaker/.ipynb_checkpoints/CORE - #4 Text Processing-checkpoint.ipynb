{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Processing Articles for Downstream Use in Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingestion of data in S3 from the CORE API stored data as JSONs with up to 100 search results stored in each file. \n",
    "Per [BlazingText Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html), the algorithm requires each line of the input file should contain a single sentence of space separated tokens. Raw data will need to be processed to accomodate the training format. \n",
    "* Before processing the raw data, a summary sheet will be created to catalog the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from io import StringIO\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re, string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer as netlem\n",
    "lem = netlem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_bucket_name = 'core0823'\n",
    "stg_bucket = 'core0823-stg'\n",
    "stg_catalog_bucket = stg_bucket + '/Catalog'\n",
    "stg_bt_bucket = stg_bucket + '/BT_STG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [i['Key'] for i in s3_client.list_objects(Bucket=core_bucket_name)['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_file_location(f_bucket, f_file):\n",
    "    \"\"\"\n",
    "    Simply returns a formatted string with the S3 file location\n",
    "    \"\"\"\n",
    "    data_location = 's3://{}/{}'.format(f_bucket,f_file)\n",
    "    return data_location\n",
    "\n",
    "def json_file_to_dict(f_bucket, f_json_file):\n",
    "    \"\"\"\n",
    "    Intakes bucket and json file.\n",
    "    Returns dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_s3_obj = s3_client.get_object( Bucket= f_bucket, Key = f_json_file )\n",
    "        tmp_str_json = json_s3_obj['Body'].read().decode('utf-8')\n",
    "        fnl_json = ast.literal_eval(tmp_str_json)\n",
    "        return fnl_json\n",
    "    except:\n",
    "        pass \n",
    "        print('Fail importing json file/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_text_parse(f_bucket, f_file_name):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and file name. \n",
    "    Parses CORE API JSON.     \n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    tmp_file = json_file_to_dict(f_bucket, f_file_name)\n",
    "    if tmp_file is not None and tmp_file['data'] is not None:\n",
    "        for item in tmp_file['data']:\n",
    "            if item['_source']['description'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['description'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "\n",
    "            if item['_source']['fullText'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['fullText'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "            \n",
    "    return results_list\n",
    "\n",
    "\n",
    "def json_extract_text(f_bucket, f_file_list):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and list of JSON files from CORE API. \n",
    "    Parses CORE API JSON. \n",
    "    This function iterates over a list of files, where json_text_parse is for a single file.\n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    print('JSON extract text starting at: {}'.format(t0))\n",
    "    results_list = []\n",
    "    for file in f_file_list:\n",
    "        tmp_results = json_text_parse(f_bucket, file)\n",
    "        results_list.extend(tmp_results)\n",
    "    \n",
    "    results_list = [sent for sent in results_list if len(sent) > 1]\n",
    "    t1 = time.time()\n",
    "    print('JSON extract text complete at: {}'.format(t1))\n",
    "    print('JSON extract text tool: {}'.format(t1-t0))\n",
    "    return results_list\n",
    "\n",
    "\n",
    "def prep_sent_list(f_sent_list):\n",
    "    \"\"\"\n",
    "    Intakes a list of sentences. \n",
    "    Uses a series of list comprehensions to prepare sentences for analysis.\n",
    "    Returns a list of sentences. \n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    print('Preparing sentences started at: {}'.format(t0))\n",
    "    sent_list = [re.sub(r'[%s]' % re.escape(string.punctuation),'',sent.lower()) for sent in f_sent_list] # make lowercase and remove punctuation\n",
    "    t1 = time.time()\n",
    "    print('Lowercase and punctuation removal completed at {}, taking {} seconds.'.format(t1,t1-t0))\n",
    "    sent_list = [re.sub(r'\\w*\\d\\w*', '',sent) for sent in f_sent_list if len( re.sub(r'\\w*\\d\\w*', '',sent) ) > 0 ] # remove words with numbers and only where non-zero length\n",
    "    t2 = time.time()\n",
    "    print('Words with numbers and zero-length removal completed at {}, taking {} seconds.'.format(t2,t2-t1))\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    for i,sent in enumerate(sent_list):\n",
    "        tmp = sent.split(' ')\n",
    "        sent_list[i] = ' '.join([lem.lemmatize(word) for word in tmp if lem.lemmatize(word) not in STOPWORDS])\n",
    "    \n",
    "    t3 = tiime.time()\n",
    "    print('Word lemmatization and stop word removal completed at {}, taking {} seconds.'.format(t2,t2-t1))\n",
    "    \n",
    "    return sent_list\n",
    "\n",
    "\n",
    "def extract_and_clean(f_bucket, f_file_list):\n",
    "    \"\"\"\n",
    "    Executes json text extraction and cleaning of text. \n",
    "    Returns a list of prep'd sentences. \n",
    "    \"\"\"\n",
    "    tmp_text = json_extract_text(f_bucket, f_file_list)\n",
    "    tmp_clean = prep_sent_list(tmp_text)\n",
    "    \n",
    "    return tmp_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON extract text starting at: 1599555295.939915\n",
      "JSON extract text complete at: 1599555684.5175335\n",
      "JSON extract text tool: 388.577618598938\n"
     ]
    }
   ],
   "source": [
    "sentences = json_extract_text(core_bucket_name, json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentences_serialized = pickle.dumps(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'FAD19FE322C66D48',\n",
       "  'HostId': 'o7Lms5X051vFIPXEmxLqSiwtYh/Bm60C54D0aoxx1wICAcpsKCRyBBS42+dcXpoEgIv/EPkQtK8=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'o7Lms5X051vFIPXEmxLqSiwtYh/Bm60C54D0aoxx1wICAcpsKCRyBBS42+dcXpoEgIv/EPkQtK8=',\n",
       "   'x-amz-request-id': 'FAD19FE322C66D48',\n",
       "   'date': 'Tue, 08 Sep 2020 09:11:15 GMT',\n",
       "   'etag': '\"8fd77685d3b62325d08ab023f7658441\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'ETag': '\"8fd77685d3b62325d08ab023f7658441\"'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client.put_object(Body=sentences_serialized, Bucket='core0823-stg', Key='BT_STG/sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "cleaned_text = extract_and_clean(core_bucket_name, json_list)\n",
    "t1 = time.time()\n",
    "print('Time to extract and prep text: {} seconds'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B C D E', 'G H I J K', 'M N O P']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['a b c d e','f g h i j k','l m n o p']\n",
    "notin = ['a','f','l']\n",
    "[' '.join([word.upper() for word in words if word not in notin]) for words in [sent.split(' ') for sent in test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time taken is  6.071857801000078\n"
     ]
    }
   ],
   "source": [
    "stmt_txt = \"\"\"\n",
    "test = ['a b c d e','f g h i j k','l m n o p']\n",
    "notin = ['a','f','l']\n",
    "[' '.join([word.upper() for word in words if word not in notin]) for words in [sent.split(' ') for sent in test]]\n",
    "\"\"\"\n",
    "print(\"The time taken is \",timeit.timeit(stmt=stmt_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time taken is  6.2610089179997885\n"
     ]
    }
   ],
   "source": [
    "stmt_txt = \"\"\"\n",
    "test = ['a b c d e','f g h i j k','l m n o p']\n",
    "notin = ['a','f','l']\n",
    "for i,sent in enumerate(test):\n",
    "    tmp = sent.split(' ')\n",
    "    test[i] = ' '.join([word.upper() for word in tmp if word not in notin])\n",
    "\"\"\"\n",
    "print(\"The time taken is \",timeit.timeit(stmt=stmt_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B C D E', 'G H I J K', 'M N O P']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['a b c d e','f g h i j k','l m n o p']\n",
    "notin = ['a','f','l']\n",
    "for i,sent in enumerate(test):\n",
    "    tmp = sent.split(' ')\n",
    "    test[i] = ' '.join([word.upper() for word in tmp if word not in notin])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
