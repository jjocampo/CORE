{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Processing Articles for Downstream Use in Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingestion of data in S3 from the CORE API stored data as JSONs with up to 100 search results stored in each file. \n",
    "Per [BlazingText Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html), the algorithm requires each line of the input file should contain a single sentence of space separated tokens. Raw data will need to be processed to accomodate the training format. \n",
    "* Before processing the raw data, a summary sheet will be created to catalog the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re, string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer as netlem\n",
    "lem = netlem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_bucket_name = 'core0823'\n",
    "stg_bucket = 'core0823-stg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [i['Key'] for i in s3_client.list_objects(Bucket=core_bucket_name)['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_file_location(f_bucket, f_file):\n",
    "    \"\"\"\n",
    "    Simply returns a formatted string with the S3 file location\n",
    "    \"\"\"\n",
    "    data_location = 's3://{}/{}'.format(f_bucket,f_file)\n",
    "    return data_location\n",
    "\n",
    "def serialobj_file_to_list(f_bucket, f_obj_key):\n",
    "    \"\"\"\n",
    "    Intakes bucket and the key for a serialized object. \n",
    "    In this case it is a serialized list object from CORE #3.\n",
    "    Returns list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_obj = s3_client.get_object( Bucket= f_bucket, Key = f_obj_key )['Body'].read()\n",
    "        return pickle.loads(s3_obj)\n",
    "    except:\n",
    "        pass \n",
    "        print('Fail getting and deserialization object.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sent_list(f_sent_list):\n",
    "    \"\"\"\n",
    "    Intakes a list of sentences. \n",
    "    Uses a series of list comprehensions to prepare sentences for analysis.\n",
    "    Returns a list of sentences. \n",
    "    \"\"\"\n",
    "    t0 = datetime.fromtimestamp( time.time() )\n",
    "    print('Preparing sentences started at: {}'.format(t0))\n",
    "    t_sent_list = [re.sub(r'[%s]' % re.escape(string.punctuation),'',sent.lower()) for sent in f_sent_list] # make lowercase and remove punctuation\n",
    "    t1 = datetime.fromtimestamp( time.time() )\n",
    "    print('Lowercase and punctuation removal completed at {}, taking {} seconds.'.format(t1, (t1-t0).total_seconds() ) )\n",
    "    t_sent_list = [re.sub(r'\\w*\\d\\w*', '',sent) for sent in t_sent_list if len( re.sub(r'\\w*\\d\\w*', '',sent) ) > 0 ] # remove words with numbers and only where non-zero length\n",
    "    t2 = datetime.fromtimestamp( time.time() )\n",
    "    print('Words with numbers and zero-length removal completed at {}, taking {} seconds.'.format(t2, (t2-t1).total_seconds() ))\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    t_sent_list = [' '.join([lem.lemmatize(word) for word in words if word not in STOPWORDS]) for words in [sent.split(' ') for sent in t_sent_list]]\n",
    "    t3 = datetime.fromtimestamp( time.time() )\n",
    "    print('Word lemmatization and stopword removal completed at {}, taking {} seconds.'.format(t3, (t3-t2).total_seconds() ))\n",
    "    \n",
    "    t4 = datetime.fromtimestamp( time.time() )\n",
    "    print('Preparing sentences completed at {}, taking a total time of {} seconds.'.format(t4, (t4-t1).total_seconds() ))\n",
    "    \n",
    "    return t_sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences imported from S3: 4140623\n"
     ]
    }
   ],
   "source": [
    "sent_list = serialobj_file_to_list(stg_bucket,'BT_STG/sentences.txt')\n",
    "print('Total sentences imported from S3: {}'.format(len(sent_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sentences 0 to 99999\n",
      "Preparing sentences started at: 2020-09-08 10:27:45.642059\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:27:47.245597, taking 1.603538 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:27:53.133917, taking 5.88832 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:27:59.414502, taking 6.280585 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:27:59.414629, taking a total time of 12.169032 seconds.\n",
      "Preparing sentences 100000 to 199999\n",
      "Preparing sentences started at: 2020-09-08 10:27:59.419429\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:28:00.957339, taking 1.53791 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:28:06.583864, taking 5.626525 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:28:12.932063, taking 6.348199 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:28:12.932191, taking a total time of 11.974852 seconds.\n",
      "Preparing sentences 200000 to 299999\n",
      "Preparing sentences started at: 2020-09-08 10:28:12.938164\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:28:14.486613, taking 1.548449 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:28:20.370951, taking 5.884338 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:28:27.275539, taking 6.904588 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:28:27.275670, taking a total time of 12.789057 seconds.\n",
      "Preparing sentences 300000 to 399999\n",
      "Preparing sentences started at: 2020-09-08 10:28:27.281849\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:28:28.793369, taking 1.51152 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:28:34.505462, taking 5.712093 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:28:40.753038, taking 6.247576 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:28:40.753190, taking a total time of 11.959821 seconds.\n",
      "Preparing sentences 400000 to 499999\n",
      "Preparing sentences started at: 2020-09-08 10:28:40.760291\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:28:42.306450, taking 1.546159 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:28:48.114860, taking 5.80841 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:28:54.704485, taking 6.589625 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:28:54.704611, taking a total time of 12.398161 seconds.\n",
      "Preparing sentences 500000 to 599999\n",
      "Preparing sentences started at: 2020-09-08 10:28:54.710003\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:28:56.269387, taking 1.559384 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:29:02.382878, taking 6.113491 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:29:09.821769, taking 7.438891 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:29:09.821895, taking a total time of 13.552508 seconds.\n",
      "Preparing sentences 600000 to 699999\n",
      "Preparing sentences started at: 2020-09-08 10:29:09.827138\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:29:11.370872, taking 1.543734 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:29:17.182612, taking 5.81174 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:29:23.718413, taking 6.535801 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:29:23.718539, taking a total time of 12.347667 seconds.\n",
      "Preparing sentences 700000 to 799999\n",
      "Preparing sentences started at: 2020-09-08 10:29:23.723904\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:29:25.282149, taking 1.558245 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:29:31.118288, taking 5.836139 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:29:38.234130, taking 7.115842 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:29:38.234259, taking a total time of 12.95211 seconds.\n",
      "Preparing sentences 800000 to 899999\n",
      "Preparing sentences started at: 2020-09-08 10:29:38.239692\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:29:39.766389, taking 1.526697 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:29:45.415166, taking 5.648777 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:29:52.381793, taking 6.966627 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:29:52.381926, taking a total time of 12.615537 seconds.\n",
      "Preparing sentences 900000 to 999999\n",
      "Preparing sentences started at: 2020-09-08 10:29:52.387296\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:29:53.963042, taking 1.575746 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:29:59.982430, taking 6.019388 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:30:07.904957, taking 7.922527 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:30:07.905345, taking a total time of 13.942303 seconds.\n",
      "Preparing sentences 1000000 to 1099999\n",
      "Preparing sentences started at: 2020-09-08 10:30:07.911027\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:30:09.544999, taking 1.633972 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:30:15.528989, taking 5.98399 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:30:22.811921, taking 7.282932 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:30:22.812053, taking a total time of 13.267054 seconds.\n",
      "Preparing sentences 1100000 to 1199999\n",
      "Preparing sentences started at: 2020-09-08 10:30:22.817475\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:30:24.430659, taking 1.613184 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:30:30.906092, taking 6.475433 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:30:38.798488, taking 7.892396 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:30:38.798619, taking a total time of 14.36796 seconds.\n",
      "Preparing sentences 1200000 to 1299999\n",
      "Preparing sentences started at: 2020-09-08 10:30:38.804103\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:30:40.380992, taking 1.576889 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:30:46.214651, taking 5.833659 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:30:53.167380, taking 6.952729 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:30:53.167719, taking a total time of 12.786727 seconds.\n",
      "Preparing sentences 1300000 to 1399999\n",
      "Preparing sentences started at: 2020-09-08 10:30:53.173055\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:30:54.750475, taking 1.57742 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:31:00.887822, taking 6.137347 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:31:08.334287, taking 7.446465 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:31:08.334700, taking a total time of 13.584225 seconds.\n",
      "Preparing sentences 1400000 to 1499999\n",
      "Preparing sentences started at: 2020-09-08 10:31:08.340316\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:31:09.917824, taking 1.577508 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:31:15.701763, taking 5.783939 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:31:22.428109, taking 6.726346 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:31:22.428243, taking a total time of 12.510419 seconds.\n",
      "Preparing sentences 1500000 to 1599999\n",
      "Preparing sentences started at: 2020-09-08 10:31:22.433768\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:31:24.014688, taking 1.58092 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:31:29.692336, taking 5.677648 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:31:36.612626, taking 6.92029 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:31:36.612757, taking a total time of 12.598069 seconds.\n",
      "Preparing sentences 1600000 to 1699999\n",
      "Preparing sentences started at: 2020-09-08 10:31:36.618268\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:31:38.184569, taking 1.566301 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:31:44.033824, taking 5.849255 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:31:51.266651, taking 7.232827 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:31:51.267044, taking a total time of 13.082475 seconds.\n",
      "Preparing sentences 1700000 to 1799999\n",
      "Preparing sentences started at: 2020-09-08 10:31:51.272841\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:31:52.879450, taking 1.606609 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:31:58.993115, taking 6.113665 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:32:06.706902, taking 7.713787 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:32:06.707042, taking a total time of 13.827592 seconds.\n",
      "Preparing sentences 1800000 to 1899999\n",
      "Preparing sentences started at: 2020-09-08 10:32:06.712788\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:32:08.342194, taking 1.629406 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:32:14.468565, taking 6.126371 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:32:21.882426, taking 7.413861 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:32:21.882558, taking a total time of 13.540364 seconds.\n",
      "Preparing sentences 1900000 to 1999999\n",
      "Preparing sentences started at: 2020-09-08 10:32:21.888286\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:32:23.442248, taking 1.553962 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:32:29.118310, taking 5.676062 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:32:35.995018, taking 6.876708 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:32:35.995148, taking a total time of 12.5529 seconds.\n",
      "Preparing sentences 2000000 to 2099999\n",
      "Preparing sentences started at: 2020-09-08 10:32:36.000766\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:32:37.614477, taking 1.613711 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:32:43.352524, taking 5.738047 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:32:50.380283, taking 7.027759 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:32:50.380414, taking a total time of 12.765937 seconds.\n",
      "Preparing sentences 2100000 to 2199999\n",
      "Preparing sentences started at: 2020-09-08 10:32:50.385951\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:32:51.933444, taking 1.547493 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:32:57.891201, taking 5.957757 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:33:05.300840, taking 7.409639 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:33:05.300988, taking a total time of 13.367544 seconds.\n",
      "Preparing sentences 2200000 to 2299999\n",
      "Preparing sentences started at: 2020-09-08 10:33:05.306698\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:33:06.913380, taking 1.606682 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:33:13.244409, taking 6.331029 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:33:21.353430, taking 8.109021 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:33:21.353822, taking a total time of 14.440442 seconds.\n",
      "Preparing sentences 2300000 to 2399999\n",
      "Preparing sentences started at: 2020-09-08 10:33:21.359545\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:33:22.900405, taking 1.54086 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:33:28.575571, taking 5.675166 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:33:36.387876, taking 7.812305 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:33:36.388014, taking a total time of 13.487609 seconds.\n",
      "Preparing sentences 2400000 to 2499999\n",
      "Preparing sentences started at: 2020-09-08 10:33:36.393853\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:33:37.929905, taking 1.536052 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:33:43.872913, taking 5.943008 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:33:51.133938, taking 7.261025 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:33:51.134069, taking a total time of 13.204164 seconds.\n",
      "Preparing sentences 2500000 to 2599999\n",
      "Preparing sentences started at: 2020-09-08 10:33:51.139966\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:33:52.690856, taking 1.55089 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:33:58.843428, taking 6.152572 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:34:06.082520, taking 7.239092 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:34:06.082657, taking a total time of 13.391801 seconds.\n",
      "Preparing sentences 2600000 to 2699999\n",
      "Preparing sentences started at: 2020-09-08 10:34:06.088422\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:34:07.666533, taking 1.578111 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:34:13.756095, taking 6.089562 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:34:21.116745, taking 7.36065 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:34:21.117142, taking a total time of 13.450609 seconds.\n",
      "Preparing sentences 2700000 to 2799999\n",
      "Preparing sentences started at: 2020-09-08 10:34:21.122795\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:34:22.654193, taking 1.531398 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:34:28.089127, taking 5.434934 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:34:34.632344, taking 6.543217 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:34:34.632478, taking a total time of 11.978285 seconds.\n",
      "Preparing sentences 2800000 to 2899999\n",
      "Preparing sentences started at: 2020-09-08 10:34:34.638027\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:34:36.164260, taking 1.526233 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:34:41.564671, taking 5.400411 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:34:47.900273, taking 6.335602 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:34:47.900404, taking a total time of 11.736144 seconds.\n",
      "Preparing sentences 2900000 to 2999999\n",
      "Preparing sentences started at: 2020-09-08 10:34:47.905973\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:34:49.440266, taking 1.534293 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:34:54.936345, taking 5.496079 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:35:01.418078, taking 6.481733 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:35:01.418502, taking a total time of 11.978236 seconds.\n",
      "Preparing sentences 3000000 to 3099999\n",
      "Preparing sentences started at: 2020-09-08 10:35:01.424541\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:35:03.018076, taking 1.593535 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:35:08.565181, taking 5.547105 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:35:14.703649, taking 6.138468 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:35:14.704016, taking a total time of 11.68594 seconds.\n",
      "Preparing sentences 3100000 to 3199999\n",
      "Preparing sentences started at: 2020-09-08 10:35:14.709731\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:35:16.295363, taking 1.585632 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:35:22.089511, taking 5.794148 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:35:28.952902, taking 6.863391 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:35:28.953047, taking a total time of 12.657684 seconds.\n",
      "Preparing sentences 3200000 to 3299999\n",
      "Preparing sentences started at: 2020-09-08 10:35:28.958590\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:35:30.509600, taking 1.55101 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:35:36.062630, taking 5.55303 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:35:42.665108, taking 6.602478 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:35:42.665237, taking a total time of 12.155637 seconds.\n",
      "Preparing sentences 3300000 to 3399999\n",
      "Preparing sentences started at: 2020-09-08 10:35:42.670800\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:35:44.233315, taking 1.562515 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:35:49.943595, taking 5.71028 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:35:56.283373, taking 6.339778 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:35:56.283496, taking a total time of 12.050181 seconds.\n",
      "Preparing sentences 3400000 to 3499999\n",
      "Preparing sentences started at: 2020-09-08 10:35:56.289716\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:35:57.849076, taking 1.55936 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:36:03.498850, taking 5.649774 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:36:10.488992, taking 6.990142 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:36:10.489128, taking a total time of 12.640052 seconds.\n",
      "Preparing sentences 3500000 to 3599999\n",
      "Preparing sentences started at: 2020-09-08 10:36:10.494667\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:36:12.071610, taking 1.576943 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:36:17.691356, taking 5.619746 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:36:24.019788, taking 6.328432 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:36:24.020181, taking a total time of 11.948571 seconds.\n",
      "Preparing sentences 3600000 to 3699999\n",
      "Preparing sentences started at: 2020-09-08 10:36:24.025821\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:36:25.572285, taking 1.546464 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:36:31.126535, taking 5.55425 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:36:37.794631, taking 6.668096 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:36:37.794761, taking a total time of 12.222476 seconds.\n",
      "Preparing sentences 3700000 to 3799999\n",
      "Preparing sentences started at: 2020-09-08 10:36:37.800825\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:36:39.335430, taking 1.534605 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:36:44.859871, taking 5.524441 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:36:51.613769, taking 6.753898 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:36:51.613941, taking a total time of 12.278511 seconds.\n",
      "Preparing sentences 3800000 to 3899999\n",
      "Preparing sentences started at: 2020-09-08 10:36:51.619634\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:36:53.195989, taking 1.576355 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:36:58.878527, taking 5.682538 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:37:05.997310, taking 7.118783 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:37:05.997443, taking a total time of 12.801454 seconds.\n",
      "Preparing sentences 3900000 to 3999999\n",
      "Preparing sentences started at: 2020-09-08 10:37:06.004118\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:37:07.584078, taking 1.57996 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:37:13.374789, taking 5.790711 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:37:19.754076, taking 6.379287 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:37:19.754208, taking a total time of 12.17013 seconds.\n",
      "Preparing sentences 4000000 to 4099999\n",
      "Preparing sentences started at: 2020-09-08 10:37:19.759818\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:37:21.328881, taking 1.569063 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:37:27.195558, taking 5.866677 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:37:34.257452, taking 7.061894 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:37:34.257580, taking a total time of 12.928699 seconds.\n"
     ]
    }
   ],
   "source": [
    "prepd_sentence = []\n",
    "sent_list_chunk = [[i-100000,i-1] for i in range(100000,len(sent_list),100000)]\n",
    "for i in sent_list_chunk:\n",
    "    print('Preparing sentences {} to {}'.format(i[0],i[1]))\n",
    "    prepd_sentence.extend( prep_sent_list(sent_list[i[0]:i[1]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepd_sentences_serialized = pickle.dumps(prepd_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.put_object(Body=prepd_sentences_serialized, Bucket='core0823-stg', Key='BT_STG/prepd_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessefully put prepd sentences into S3.\n"
     ]
    }
   ],
   "source": [
    "if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "    print('Sucessefully put prepd sentences into S3.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Discovering how the brain works is perhaps the most extraordinary scientific challenge of our time.',\n",
       " 'Advances in understanding the brain will inform medical research into new treatments for neurological disorders, as well as lead to powerful new techniques in artificial intelligence and robot control.',\n",
       " 'To meet this challenge, our foundation is raising funds to support a new Centre for Theoretical Neuroscience at Oxford, which will be dedicated to teaching and research in computer modelling of the brain.',\n",
       " 'The Centre is currently based within the Oxford University Department of Experimental Psychology.',\n",
       " 'Over the last year, we have made important contributions to understanding various areas of brain function, including for example: How do our visual systems learn to make sens']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['discovering brain work perhaps extraordinary scientific challenge time',\n",
       " 'advance understanding brain inform medical research new treatment neurological disorder well lead powerful new technique artificial intelligence robot control',\n",
       " 'meet challenge foundation raising fund support new centre theoretical neuroscience oxford dedicated teaching research computer modelling brain',\n",
       " 'centre currently based within oxford university department experimental psychology',\n",
       " 'last year made important contribution understanding various area brain function including example visual system learn make sen']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepd_sentence[:5]"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
