{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Processing Articles for Downstream Use in Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingestion of data in S3 from the CORE API stored data as JSONs with up to 100 search results stored in each file. \n",
    "Per [BlazingText Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html), the algorithm requires each line of the input file should contain a single sentence of space separated tokens. Raw data will need to be processed to accomodate the training format. \n",
    "* Before processing the raw data, a summary sheet will be created to catalog the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re, string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer as netlem\n",
    "lem = netlem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_bucket_name = 'core0823'\n",
    "stg_bucket = 'core0823-stg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [i['Key'] for i in s3_client.list_objects(Bucket=core_bucket_name)['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_file_location(f_bucket, f_file):\n",
    "    \"\"\"\n",
    "    Simply returns a formatted string with the S3 file location\n",
    "    \"\"\"\n",
    "    data_location = 's3://{}/{}'.format(f_bucket,f_file)\n",
    "    return data_location\n",
    "\n",
    "def serialobj_file_to_list(f_bucket, f_obj_key):\n",
    "    \"\"\"\n",
    "    Intakes bucket and the key for a serialized object. \n",
    "    In this case it is a serialized list object from CORE #3.\n",
    "    Returns list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_obj = s3_client.get_object( Bucket= f_bucket, Key = f_obj_key )['Body'].read()\n",
    "        return pickle.loads(s3_obj)\n",
    "    except:\n",
    "        pass \n",
    "        print('Fail getting and deserialization object.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sent_list(f_sent_list):\n",
    "    \"\"\"\n",
    "    Intakes a list of sentences. \n",
    "    Uses a series of list comprehensions to prepare sentences for analysis.\n",
    "    Returns a list of sentences. \n",
    "    \"\"\"\n",
    "    t0 = datetime.fromtimestamp( time.time() )\n",
    "    print('Preparing sentences started at: {}'.format(t0))\n",
    "    t_sent_list = [re.sub(r'[%s]' % re.escape(string.punctuation),'',sent.lower()) for sent in f_sent_list] # make lowercase and remove punctuation\n",
    "    t1 = datetime.fromtimestamp( time.time() )\n",
    "    print('Lowercase and punctuation removal completed at {}, taking {} seconds.'.format(t1, (t1-t0).total_seconds() ) )\n",
    "    t_sent_list = [re.sub(r'\\w*\\d\\w*', '',sent) for sent in t_sent_list if len( re.sub(r'\\w*\\d\\w*', '',sent) ) > 0 ] # remove words with numbers and only where non-zero length\n",
    "    t2 = datetime.fromtimestamp( time.time() )\n",
    "    print('Words with numbers and zero-length removal completed at {}, taking {} seconds.'.format(t2, (t2-t1).total_seconds() ))\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    t_sent_list = [' '.join([lem.lemmatize(word) for word in words if word not in STOPWORDS]) for words in [sent.split(' ') for sent in t_sent_list]]\n",
    "    t3 = datetime.fromtimestamp( time.time() )\n",
    "    print('Word lemmatization and stopword removal completed at {}, taking {} seconds.'.format(t3, (t3-t2).total_seconds() ))\n",
    "    \n",
    "    t4 = datetime.fromtimestamp( time.time() )\n",
    "    print('Preparing sentences completed at {}, taking a total time of {} seconds.'.format(t4, (t4-t1).total_seconds() ))\n",
    "    \n",
    "    return t_sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences imported from S3: 4140623\n"
     ]
    }
   ],
   "source": [
    "sent_list = serialobj_file_to_list(stg_bucket,'BT_STG/sentences.txt')\n",
    "print('Total sentences imported from S3: {}'.format(len(sent_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sentences 0 to 99999\n",
      "Preparing sentences started at: 2020-09-08 10:27:45.642059\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:27:47.245597, taking 1.603538 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:27:53.133917, taking 5.88832 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:27:59.414502, taking 6.280585 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:27:59.414629, taking a total time of 12.169032 seconds.\n",
      "Preparing sentences 100000 to 199999\n",
      "Preparing sentences started at: 2020-09-08 10:27:59.419429\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:28:00.957339, taking 1.53791 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:28:06.583864, taking 5.626525 seconds.\n",
      "Word lemmatization and stopword removal completed at 2020-09-08 10:28:12.932063, taking 6.348199 seconds.\n",
      "Preparing sentences completed at 2020-09-08 10:28:12.932191, taking a total time of 11.974852 seconds.\n",
      "Preparing sentences 200000 to 299999\n",
      "Preparing sentences started at: 2020-09-08 10:28:12.938164\n",
      "Lowercase and punctuation removal completed at 2020-09-08 10:28:14.486613, taking 1.548449 seconds.\n",
      "Words with numbers and zero-length removal completed at 2020-09-08 10:28:20.370951, taking 5.884338 seconds.\n"
     ]
    }
   ],
   "source": [
    "prepd_sentence = []\n",
    "sent_list_chunk = [[i-100000,i-1] for i in range(100000,len(sent_list),100000)]\n",
    "for i in sent_list_chunk:\n",
    "    print('Preparing sentences {} to {}'.format(i[0],i[1]))\n",
    "    prepd_sentence.extend( prep_sent_list(sent_list[i[0]:i[1]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepd_sentences = prep_sent_list(sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepd_sentences_serialized = pickle.dumps(prepd_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.put_object(Body=sentences_serialized, Bucket='core0823-stg', Key='BT_STG/prepd_sentences.txt')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
