{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Processing Articles for Downstream Use in Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingestion of data in S3 from the CORE API stored data as JSONs with up to 100 search results stored in each file. \n",
    "Per [BlazingText Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html), the algorithm requires each line of the input file should contain a single sentence of space separated tokens. Raw data will need to be processed to accomodate the training format. \n",
    "* Before processing the raw data, a summary sheet will be created to catalog the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from io import StringIO\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re, string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer as netlem\n",
    "lem = netlem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_bucket_name = 'core0823'\n",
    "stg_bucket = 'core0823-stg'\n",
    "stg_catalog_bucket = stg_bucket + '/Catalog'\n",
    "stg_bt_bucket = stg_bucket + '/BT_STG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [i['Key'] for i in s3_client.list_objects(Bucket=core_bucket_name)['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_file_location(f_bucket, f_file):\n",
    "    \"\"\"\n",
    "    Simply returns a formatted string with the S3 file location\n",
    "    \"\"\"\n",
    "    data_location = 's3://{}/{}'.format(f_bucket,f_file)\n",
    "    return data_location\n",
    "\n",
    "def json_file_to_dict(f_bucket, f_json_file):\n",
    "    \"\"\"\n",
    "    Intakes bucket and json file.\n",
    "    Returns dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_s3_obj = s3_client.get_object( Bucket= f_bucket, Key = f_json_file )\n",
    "        tmp_str_json = json_s3_obj['Body'].read().decode('utf-8')\n",
    "        fnl_json = ast.literal_eval(tmp_str_json)\n",
    "        return fnl_json\n",
    "    except:\n",
    "        pass \n",
    "        print('Fail importing json file/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_text_parse(f_bucket, f_file_name):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and file name. \n",
    "    Parses CORE API JSON.     \n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    tmp_file = json_file_to_dict(f_bucket, f_file_name)\n",
    "    if tmp_file is not None and tmp_file['data'] is not None:\n",
    "        for item in tmp_file['data']:\n",
    "            if item['_source']['description'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['description'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "\n",
    "            if item['_source']['fullText'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['fullText'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "            \n",
    "    return results_list\n",
    "\n",
    "\n",
    "def json_extract_text(f_bucket, f_file_list):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and list of JSON files from CORE API. \n",
    "    Parses CORE API JSON. \n",
    "    This function iterates over a list of files, where json_text_parse is for a single file.\n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    for file in f_file_list:\n",
    "        tmp_results = json_text_parse(f_bucket, file)\n",
    "        results_list.extend(tmp_results)\n",
    "    \n",
    "    results_list = [sent for sent in results_list if len(sent) > 1]\n",
    "    \n",
    "    return results_list\n",
    "\n",
    "\n",
    "def prep_sent_list(f_sent_list):\n",
    "    \"\"\"\n",
    "    Intakes a list of sentences. \n",
    "    Uses a series of list comprehensions to prepare sentences for analysis.\n",
    "    Returns a list of sentences. \n",
    "    \"\"\"\n",
    "    sent_list = [re.sub(r'[%s]' % re.escape(string.punctuation),'',sent.lower()) for sent in f_sent_list] # make lowercase and remove punctuation\n",
    "    sent_list = [re.sub(r'\\w*\\d\\w*', '',sent) for sent in f_sent_list if len( re.sub(r'\\w*\\d\\w*', '',sent) ) > 0 ] # remove words with numbers and only where non-zero length\n",
    "\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    for i,sent in enumerate(sent_list):\n",
    "        tmp = sent.split(' ')\n",
    "        sent_list[i] = ' '.join([lem.lemmatize(word) for word in tmp if lem.lemmatize(word) not in STOPWORDS])\n",
    "    \n",
    "    return sent_list\n",
    "\n",
    "\n",
    "def extract_and_clean(f_bucket, f_file_list):\n",
    "    \"\"\"\n",
    "    Executes json text extraction and cleaning of text. \n",
    "    Returns a list of prep'd sentences. \n",
    "    \"\"\"\n",
    "    tmp_text = json_extract_text(f_bucket, f_file_list)\n",
    "    tmp_clean = prep_sent_list(tmp_text)\n",
    "    \n",
    "    return tmp_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "cleaned_text = extract_and_clean(core_bucket_name, json_list)\n",
    "t1 = time.time()\n",
    "print('Time to extract and prep text: {} seconds'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
