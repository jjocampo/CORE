{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Processing Articles for Downstream Use in Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingestion of data in S3 from the CORE API stored data as JSONs with up to 100 search results stored in each file. \n",
    "Per [BlazingText Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html), the algorithm requires each line of the input file should contain a single sentence of space separated tokens. Raw data will need to be processed to accomodate the training format. \n",
    "* Before processing the raw data, a summary sheet will be created to catalog the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from io import StringIO\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re, string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORD = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer as netlem\n",
    "lem = netlem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_bucket_name = 'core0823'\n",
    "stg_bucket = 'core0823-stg'\n",
    "stg_catalog_bucket = stg_bucket + '/Catalog'\n",
    "stg_bt_bucket = stg_bucket + '/BT_STG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [i['Key'] for i in s3_client.list_objects(Bucket=core_bucket_name)['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_file_location(f_bucket, f_file):\n",
    "    \"\"\"\n",
    "    Simply returns a formatted string with the S3 file location\n",
    "    \"\"\"\n",
    "    data_location = 's3://{}/{}'.format(f_bucket,f_file)\n",
    "    return data_location\n",
    "\n",
    "def json_file_to_dict(f_bucket, f_json_file):\n",
    "    \"\"\"\n",
    "    Intakes bucket and json file.\n",
    "    Returns dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_s3_obj = s3_client.get_object( Bucket= f_bucket, Key = f_json_file )\n",
    "        tmp_str_json = json_s3_obj['Body'].read().decode('utf-8')\n",
    "        fnl_json = ast.literal_eval(tmp_str_json)\n",
    "        return fnl_json\n",
    "    except:\n",
    "        pass \n",
    "        print('Fail importing json file/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_text_parse(f_bucket, f_file_name):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and file name. \n",
    "    Parses CORE API JSON.     \n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    tmp_file = json_file_to_dict(f_bucket, f_file_name)\n",
    "    if tmp_file is not None and tmp_file['data'] is not None:\n",
    "        for item in tmp_file['data']:\n",
    "            if item['_source']['description'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['description'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "\n",
    "            if item['_source']['fullText'] is not None:\n",
    "                tmp_parse_list = tokenize.sent_tokenize(item['_source']['fullText'])\n",
    "                results_list.extend(tmp_parse_list)\n",
    "            \n",
    "    return results_list\n",
    "\n",
    "def json_extract_text(f_bucket, f_file_list):\n",
    "    \"\"\"\n",
    "    Intakes a bucket and list of JSON files from CORE API. \n",
    "    Parses CORE API JSON. \n",
    "    This function iterates over a list of files, where json_text_parse is for a single file.\n",
    "    Returns list of lists, where each entry is a sentence.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    for file in f_file_list:\n",
    "        tmp_results = json_text_parse(f_bucket, file)\n",
    "        results_list.extend(tmp_results)\n",
    "    \n",
    "    results_list = [sent for sent in results_list if len(sent) > 1]\n",
    "    \n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func2 = json_extract_text(core_bucket_name, json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5281986"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The distorted wave including up to 25(th) harmonics were prepared for testing of the neural networks.',\n",
       " \"Elman's recurrent and feed forward neural networks were used to recognize each harmonic.\",\n",
       " \"The results obtained using Elman's recurrent neural networks are better than the results values obtained using the feed forward neural networks for resilient back propagation\",\n",
       " 'The junction of AI and computer security is an area of increasing concern, due to the imminent application of AI to fielded systems.',\n",
       " 'Two new areas of research need are identified: artificial intelligence techniques in the development of secure systems.']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_func2[300000:300005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func3 = [sent for sent in test_func2 if len(sent) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4140623"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_func3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sent_list(f_sent_list):\n",
    "    \"\"\"\n",
    "    Intakes a list of sentences. \n",
    "    Uses a series of list comprehensions to prepare sentences for analysis.\n",
    "    Returns a list of sentences. \n",
    "    \"\"\"\n",
    "    sent_list = [re.sub(r'[%s]' % re.escape(string.punctuation),'',sent.lower()) for sent in f_sent_list] # make lowercase and remove punctuation\n",
    "    sent_list = [re.sub(r'\\w*\\d\\w*','',sent) for sent in sent_list] # remove words with numbers in them\n",
    "    sent_list = [sent for sent in sent_list if len(sent) > 0] # remove any zero length sentences \n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    for i,sent in enumerate(sent_list):\n",
    "        tmp = sent.split(' ')\n",
    "        sent_list[i] = ' '.join([lem.lemmatize(word) for word in tmp if lem.lemmatize(word) not in STOPWORDS])\n",
    "    \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func4 = prep_sent_list(test_func3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test = [' '.join([lem.lemmatize(word) for word in [sent.lower().split(' ') for sent in test_func3[:1000]] if lem.lemmatize(word) not in STOPWORDS])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['discovering how the brain works is perhaps the most extraordinary scientific challenge of our time',\n",
       " 'advances in understanding the brain will inform medical research into new treatments for neurological disorders as well as lead to powerful new techniques in artificial intelligence and robot control',\n",
       " 'to meet this challenge our foundation is raising funds to support a new centre for theoretical neuroscience at oxford which will be dedicated to teaching and research in computer modelling of the brain',\n",
       " 'the centre is currently based within the oxford university department of experimental psychology',\n",
       " 'over the last year we have made important contributions to understanding various areas of brain function including for example how do our visual systems learn to make sens']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_func4[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
